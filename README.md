# ðŸ” NGram-DetectGPT: AI Text Detection using N-Gram Delta Features

This project implements a robust detection pipeline to distinguish AI-generated text from human-written content using classical n-gram language models (KenLM) and perturbation-based delta features. It includes paraphrase generation, n-gram scoring, feature extraction, classifier training, evaluation, and result visualization.

---

## ðŸ“ Repository Structure

```
Ngram_DetectGPT/
â”œâ”€â”€ generate_variants_chatgpt.py      # Paraphrase generation using GPT-4 API
â”œâ”€â”€ extract_features_kenlm.py         # Computes n-gram delta features
â”œâ”€â”€ train_classifier.py               # Trains XGBoost classifiers
â”œâ”€â”€ eval_results.py                   # Aggregates and visualizes evaluation metrics
â”œâ”€â”€ length_robustness.py              # Plots AUC vs. text length
â”œâ”€â”€ ablation_study.py                 # Runs feature ablation experiments
â”œâ”€â”€ plot_figures.py                   # Creates accuracy and AUC plots
â”œâ”€â”€ models/                           # Pretrained KenLM models (2-gram to 5-gram)
â”œâ”€â”€ datasets/                         # Input CSVs (original texts)
â”œâ”€â”€ output/                           # Paraphrased variants per temperature
â”œâ”€â”€ N-gram Scoring/                   # Feature CSVs with delta scores
â”œâ”€â”€ results/                          # Evaluation metrics, plots, summaries
â””â”€â”€ README.md
```

---

## ðŸ§° Installation Instructions

### 1. âœ… Clone the Repository

```bash
git clone https://github.com/N-Gram-dev/Ngram_DetectGPT.git
cd Ngram_DetectGPT
```

### 2. âœ… Create and Activate Virtual Environment (Optional but Recommended)

```bash
python3 -m venv env
source env/bin/activate      # On Windows: env\Scripts\activate
```

### 3. âœ… Install Python Dependencies

```bash
pip install -r requirements.txt
```

If you donâ€™t have the `requirements.txt` yet, create one with the content below ðŸ‘‡

---

## ðŸ“¦ `requirements.txt`

```text
pandas
numpy
scipy
nltk
openai
tqdm
kenlm
xgboost
scikit-learn
matplotlib
seaborn
```

---

## ðŸ“¥ Download Pretrained KenLM Models

You must download 4 pre-trained n-gram models (2-gram to 5-gram) into a folder named `models/` in the repo root.

### ðŸ“ Source:
ðŸ‘‰ https://huggingface.co/NGramDev/ngram-detect-models

### ðŸ§¾ Files to download:

- `2-gram.arpa.bin`
- `3-gram.arpa.bin`
- `4-gram.arpa.bin`
- `5-gram.arpa.bin`

### ðŸ“ Final Structure:

```
models/
â”œâ”€â”€ 2-gram.arpa.bin
â”œâ”€â”€ 3-gram.arpa.bin
â”œâ”€â”€ 4-gram.arpa.bin
â””â”€â”€ 5-gram.arpa.bin
```

> These binary KenLM files are used for log-likelihood, entropy, and variance-based delta feature extraction.

---

## ðŸš¦ How to Run the Pipeline

The following scripts should be run in order:

1. `generate_variants_chatgpt.py` â€“ Calls OpenAI GPT-4 API to generate 10 paraphrases per sentence at 6 temperatures.
2. `extract_features_kenlm.py` â€“ Computes log-probability, entropy, and frequency variance deltas using KenLM models.
3. `train_classifier.py` â€“ Trains XGBoost classifiers per dataset/model/temperature using extracted delta features.
4. `eval_results.py` â€“ Aggregates performance across all runs and produces summary plots.
5. `plot_figures.py` â€“ Plots accuracy, AUC, and distributions across variants.
6. `ablation_study.py` â€“ Tests the impact of removing feature groups (log-score, entropy, frequency).
7. `length_robustness.py` â€“ Analyzes detection AUC stability across different passage lengths.

> ðŸ“‚ Output CSVs, ROC-AUC scores, plots, confusion matrices, and summaries are automatically saved in the `results/` folder.

---


## âœ… You're Ready to Go!

Once youâ€™ve installed the requirements and downloaded the models:
- Run each script step-by-step.
- Outputs will be auto-saved in structured folders.
- All results, including variant data, delta features, AUCs, and visualizations, will be available for analysis.

Need help? Open an issue or refer to the scripts' internal comments for guidance.
